{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Chapter 3: Classification of land cover\n",
    "======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "+ <b>To use scikit learn libraries to classify the Landsat image using python.</b>\n",
    "+ <b>Learn about Random forest algorithm and implement it in python on satellite imagery.</b>\n",
    "\n",
    "\n",
    "\n",
    "## What is a Random forest?\n",
    "A brief explanation of the RandomForest algorithm comes from the name. Rather than utilize the predictions of a single decision tree, the algorithm will take the ensemble result of a large number of decision trees (a forest of them). The \"Random\" part of the name comes from the term \"bootstrap aggregating\", or \"bagging\". What this means is that each tree within the forest only gets to train on some subset of the full training dataset (the subset is determined by sampling with replacement). The elements of the training data for each tree that are left unseen are held \"out-of-bag\" for estimation of accuracy. Randomness also helps decide which feature input variables are seen at each node in each decision tree. Once all individual trees are fit to the random subset of the training data, using a random set of feature variable at each node, the ensemble of them all is used to give the final prediction. \n",
    "\n",
    "#### Based off of decision trees\n",
    "![alt text](http://docsdrive.com/images/ansinet/itj/2009/fig3-2k8-5841.gif \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "#### Forest: Collection of trees\n",
    "![alt text](https://i.ytimg.com/vi/ajTc5y3OqSQ/hqdefault.jpg \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "### But how do we construct the optimal tree? \n",
    "\n",
    "We use the Gini Index as our cost function used to evaluate splits in the dataset. We minimize it.\n",
    "\n",
    "A split in the dataset involves one input attribute and one value for that attribute. It can be used to divide training patterns into two groups of rows.\n",
    "\n",
    "A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created \n",
    "by the split. A perfect separation results in a Gini score of 0, whereas the worst case split that \n",
    "results in 50/50 classes in each group results in a Gini score of 1.0 (for a 2 class problem). We calculate it for every row and split the data accordingly in our binary tree. We repeat this process recursively. \n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/decisiontree-151015165353-lva1-app6892/95/classification-using-decision-tree-41-638.jpg?cb=1444928106 \"Logo Title Text 1\")\n",
    " \n",
    "\n",
    "\n",
    "### Advantages of Random Forest:\n",
    "+ It gives you a measure of \"variable important\" which relates how useful your input features (e.g. spectral bands) were in the classification\n",
    "+ The \"out-of-bag\" samples in each tree can be used to validate each tree. Grouping these predicted accuracies across all trees can [sometimes give you an unbiased estimate of the error rate](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr) similar to doing cross-validation.\n",
    "+ Can be used for regressions, unsupervised clustering, or supervised classification\n",
    "+ Available in many popular languages, including Python, R, and MATLAB\n",
    "+ Free and open source, and fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the classification mode, this means that if you were to have 5 classes being predicted using 500 trees, the output prediction would be the class that has the most number of the 500 trees predicting it. The proportion of the number of trees that voted for the winning class can be a diagnostic of the representativeness of your training data relative to the rest of the image. Taking the 500 trees example, if you have pixels which are voted to be in the \"Forest\" land cover class by 475 of 500 trees, you could say that this was a relatively certain prediction. On the other hand, if you have a pixel which gets 250 votes for \"Forest\" and 225 votes for \"Shrub\", you could interpret this as either an innately confusing pixel (maybe it is a mixed pixel, or it is a small statured forest) or as an indicator that you need more training data samples in these types of pixels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries used\n",
    "\n",
    "- GDAL\n",
    "- Numpy\n",
    "- Matplotlib\n",
    "- Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "#### Opening the images\n",
    "Reading in the example image and the ROI image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python 3's print function and division\n",
    "#from __future__ import print_function, division\n",
    "\n",
    "# Import GDAL, NumPy, and matplotlib\n",
    "from osgeo import gdal, gdal_array\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in our image and ROI image\n",
    "img_ds = gdal.Open('../../example/LE70220491999322EDC01_stack.gtif', gdal.GA_ReadOnly)\n",
    "roi_ds = gdal.Open('../../example/training_data.gtif', gdal.GA_ReadOnly)\n",
    "\n",
    "img = np.zeros((img_ds.RasterYSize, img_ds.RasterXSize, img_ds.RasterCount),\n",
    "               gdal_array.GDALTypeCodeToNumericTypeCode(img_ds.GetRasterBand(1).DataType))\n",
    "for b in range(img.shape[2]):\n",
    "    img[:, :, b] = img_ds.GetRasterBand(b + 1).ReadAsArray()\n",
    "    \n",
    "roi = roi_ds.GetRasterBand(1).ReadAsArray().astype(np.uint8)\n",
    "\n",
    "# Display them\n",
    "plt.subplot(121)\n",
    "plt.imshow(img[:, :, 4])\n",
    "plt.title('SWIR1')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(roi)\n",
    "plt.title('ROI Training Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairing Y with X\n",
    "Now that we have the image we want to classify (our X feature inputs), and the ROI with the land cover labels (our Y labeled data), we need to pair them up in NumPy arrays so we may feed them to Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many non-zero entries we have -- i.e. how many training data samples?\n",
    "n_samples = (roi > 0).sum()\n",
    "print('We have {n} samples'.format(n=n_samples))\n",
    "\n",
    "# What are our classification labels?\n",
    "labels = np.unique(roi[roi > 0])\n",
    "print('The training data include {n} classes: {classes}'.format(n=labels.size, \n",
    "                                                                classes=labels))\n",
    "# We will need a \"X\" matrix containing our features, and a \"y\" array containing our labels\n",
    "#     These will have n_samples rows\n",
    "#     In other languages we would need to allocate these and them loop to fill them, but NumPy can be faster\n",
    "\n",
    "X = img[roi > 0, :]  # include 8th band, which is Fmask, for now\n",
    "y = roi[roi > 0]\n",
    "\n",
    "print('Our X matrix is sized: {sz}'.format(sz=X.shape))\n",
    "print('Our y array is sized: {sz}'.format(sz=y.shape))\n",
    "\n",
    "# Mask out clouds, cloud shadows, and snow using Fmask\n",
    "clear = X[:, 7] <= 1\n",
    "\n",
    "X = X[clear, :7]  # we can ditch the Fmask band now\n",
    "\n",
    "y = y[clear]\n",
    "print('After masking, our X matrix is sized: {sz}'.format(sz=X.shape))\n",
    "print('After masking, our y array is sized: {sz}'.format(sz=y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Random Forest\n",
    "Now that we have our X matrix of feature inputs (the spectral bands) and our y array (the labels), we can train our model.\n",
    "\n",
    "Visit [this web page to find the usage of RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize our model with 500 trees\n",
    "rf = RandomForestClassifier(n_estimators=500, oob_score=True)\n",
    "\n",
    "# Fit our model to training data\n",
    "rf = rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our Random Forest model fit, we can check out the \"Out-of-Bag\" (OOB) prediction score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help us get an idea of which spectral bands were important, we can look at the feature importance scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [1, 2, 3, 4, 5, 7, 6]\n",
    "\n",
    "for b, imp in zip(bands, rf.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the largest weights, it looks like the SWIR1 and the Green bands were the most useful to us. Not surprising, the thermal band is not very useful because there isn't much delineation of land cover type with temperature if you only look with a very small area not influenced by Urban Heat Island.\n",
    "\n",
    "Let's look at a crosstabulation to see the class confusion. To do so, we will import the [Pandas](http://pandas.pydata.org/) library for some help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Setup a dataframe -- just like R\n",
    "df = pd.DataFrame()\n",
    "df['truth'] = y\n",
    "df['predict'] = rf.predict(X)\n",
    "\n",
    "\n",
    "# Cross-tabulate predictions\n",
    "print(pd.crosstab(df['truth'], df['predict'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the rest of the image\n",
    "\n",
    "With our Random Forest classifier fit, we can now proceed by trying to classify the entire image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take our full image, ignore the Fmask band, and reshape into long 2d array (nrow * ncol, nband) for classification\n",
    "new_shape = (img.shape[0] * img.shape[1], img.shape[2] - 1)\n",
    "\n",
    "img_as_array = img[:, :, :7].reshape(new_shape)\n",
    "print('Reshaped from {o} to {n}'.format(o=img.shape,\n",
    "                                        n=img_as_array.shape))\n",
    "\n",
    "# Now predict for each pixel\n",
    "class_prediction = rf.predict(img_as_array)\n",
    "\n",
    "# Reshape our classification map\n",
    "class_prediction = class_prediction.reshape(img[:, :, 0].shape)\n",
    "print(class_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "\n",
    "# First setup a 5-4-3 composite\n",
    "def color_stretch(image, index, minmax=(0, 10000)):\n",
    "    colors = image[:, :, index].astype(np.float64)\n",
    "\n",
    "    max_val = minmax[1]\n",
    "    min_val = minmax[0]\n",
    "\n",
    "    # Enforce maximum and minimum values\n",
    "    colors[colors[:, :, :] > max_val] = max_val\n",
    "    colors[colors[:, :, :] < min_val] = min_val\n",
    "\n",
    "    for b in range(colors.shape[2]):\n",
    "        colors[:, :, b] = colors[:, :, b] * 1 / (max_val - min_val)\n",
    "        \n",
    "    return colors\n",
    "    \n",
    "img543 = color_stretch(img, [4, 3, 2], (0, 8000))\n",
    "\n",
    "# See https://github.com/matplotlib/matplotlib/issues/844/\n",
    "n = class_prediction.max()\n",
    "# Next setup a colormap for our map\n",
    "colors = dict((\n",
    "    (0, (0, 0, 0, 255)),  # Nodata\n",
    "    (1, (0, 150, 0, 255)),  # Forest\n",
    "    (2, (0, 0, 255, 255)),  # Water\n",
    "    (3, (0, 255, 0, 255)),  # Herbaceous\n",
    "    (4, (160, 82, 45, 255)),  # Barren\n",
    "    (5, (255, 0, 0, 255))  # Urban\n",
    "))\n",
    "# Put 0 - 255 as float 0 - 1\n",
    "for k in colors:\n",
    "    v = colors[k]\n",
    "    _v = [_v / 255.0 for _v in v]\n",
    "    colors[k] = _v\n",
    "    \n",
    "index_colors = [colors[key] if key in colors else \n",
    "                (255, 255, 255, 0) for key in range(1, n + 1)]\n",
    "cmap = plt.matplotlib.colors.ListedColormap(index_colors, 'Classification', n)\n",
    "\n",
    "# Now show the classmap next to the image\n",
    "plt.subplot(121)\n",
    "plt.imshow(img543)\n",
    "plt.title('Landsat Image')\n",
    "print(class_prediction.shape)\n",
    "plt.subplot(122)\n",
    "plt.imshow(class_prediction, cmap=cmap, interpolation='none')\n",
    "plt.title('Classified Image')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapup\n",
    "\n",
    "We've seen how we can use `scikit-learn` to implement the Random Forest classifier for land cover classification. \n",
    "\n",
    "We've seen how Random Forest can come up with an estimate of the classification accuracy using the \"Out-of-Bag\" samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
